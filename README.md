# 虎鲸笔记 Logseq 导入插件 (v1.2) - 开发全复盘与交接文档

## 1. 项目目标 (它是什么？)
本项目的核心目标是：开发一个虎鲸笔记插件，使用户能够将他们现有的 Logseq 笔记库，方便、无损地迁移到虎鲸笔记中。

## 2. 当前状态 (它现在能做什么？)
**核心架构稳定，但内容保真度低，尚不具备生产环境使用条件。**

*   **能做到的**:
    *   插件能够被虎鲸笔记正确加载和运行。
    *   提供了一个基础的图形化界面，让用户可以选择本地的 Logseq 文件夹。
    *   能够解析 Logseq 的 `.md` 文件，并识别出笔记的层级结构。
    *   **成功解决了**因大量调用 API 导致的程序崩溃问题，采用了批量、原子化的方式进行数据写入，保证了在中小型笔记库导入时的**稳定性**。

*   **做不到的 (核心问题)**:
    *   无法正确转换 Logseq 中的大部分特殊格式。**块引用、块嵌入、附件链接、PDF链接等都会丢失格式**，被降级为无格式的纯文本占位符。这导致了迁移后的笔记内容可读性很差，失去了原有的链接和上下文关系。

**效果对比:**

（下图左侧为Logseq原始笔记，右侧为本插件导入虎-鲸后的不佳效果）
[图片对比]

<img width="3379" height="1438" alt="PixPin_2025-08-23_23-33-25" src="https://github.com/user-attachments/assets/c7ace894-f107-4ba1-a5bc-9a61168ded23" />

*这是一个占位符，实际使用时应替换为效果对比图*

**结论：本项目目前暂停，将所有经验和未解决的问题记录于此，供后续开发者参考。**

## 3. 如何安装与使用 (怎么用？)
1.  **下载代码**: 获取本项目的所有文件。
2.  **安装依赖**: 在 `orca-logseq-importer` 目录下，运行 `pnpm install`。
3.  **编译插件**: 运行 `pnpm run build`。这会在 `orca-logseq-importer` 目录下生成一个 `dist` 文件夹。
4.  **安装插件**: 将整个 `orca-logseq-importer` 文件夹（必须包含 `dist` 目录和 `icon.png` 等文件）复制到虎鲸笔记的 `plugins` 目录中 (可通过 `设置` > `关于` > `数据存储路径` 找到)。
5.  **重启虎鲸**: **完全退出并重启**虎鲸笔记应用。
6.  **运行导入**:
    *   在 `设置` > `插件` 中确保 "Logseq Importer" 已启用。
    *   通过命令面板 (`Ctrl+P` 或 `Cmd+P`) 运行 **"Logseq: 开始导入"** 命令。
    *   在弹出的界面中选择您的 Logseq 笔记库根文件夹。
    *   插件将开始导入，并通过右下角通知反馈进度。

## 4. 项目结构与核心文件解读 (文件结构是怎样的？)
*   `package.json`: 定义了项目依赖（如 `vite`, `typescript`）和关键脚本（如 `pnpm run build`）。是项目的“身份证”。
*   `vite.config.ts`: Vite 的配置文件，用于构建和打包插件。
*   `dist/index.js`: **最终产物**。这是由 `pnpm run build` 生成的、实际被虎鲸笔记加载的插件文件。
*   `src/main.ts`: **插件主入口**。负责注册命令、管理插件的 `load` 和 `unload` 生命周期。用户交互的起点（“开始导入”命令）就在这里定义。
*   `src/ui.tsx`: **前端UI组件**。使用 React 编写，构建了用户选择文件夹的模态框界面。
*   `src/parser.ts`: **核心解析器**。负责读取 `.md` 文件内容，将其解析成结构化的 `LogseqPage` 和 `LogseqBlock` 对象，并建立 `uuid -> block` 的映射，是数据处理的第一步。
*   `src/importer.ts`: **核心导入器**。包含最重要的 `importPageBatch` 和 `parseContentToFragments` 函数。它的职责是：
    1.  接收解析器处理好的结构化数据。
    2.  将 Logseq 块数据（特别是内容字符串）转换为虎鲸 `batchInsertReprs` API 所需的 `Repr[]` 格式。**（这是当前效果不佳的根源，也是后续工作的核心）**
    3.  调用虎鲸的批量API，将数据写入数据库。
*   `src/orca.d.ts`: 虎鲸笔记官方的API类型定义文件，是理解所有API和数据结构最权威的参考。

## 5. 开发经验与教训总结 (我们学到了什么？)
*   **环境是黑盒**: 虎鲸的插件运行环境与标准Web开发环境差异巨大。**绝对不要**假设标准的 `npm` 包装和模块解析能够正常工作。
*   **依赖全局变量**: **首选方案**是直接使用挂载在 `window` 对象上的全局变量，如 `window.React`、`window.createRoot` 等。这是最稳定、最可靠的方式。
*   **谨慎处理API调用**: 虎鲸的后端API似乎有**速率限制**。过于频繁地调用（即使是不同的命令），也可能导致 `MaxTrialCount` 错误。应尽可能将多个操作合并到一次API调用中，例如使用 `batchInsertReprs` 而不是多次调用 `insertBlock`。
*   **架构演进**:
    1.  **失败的 v1.0 (逐块插入)**: 逐个调用 `core.editor.insertBlock`。**结果**：因 `TypeError: (intermediate value) is not iterable` 而完全阻塞。
    2.  **失败的 v1.1 (逐页打包)**: 采用 `batchInsertReprs` 解决了 `TypeError`，但因对每个页面都调用一次 `invokeGroup`，在处理大量页面时触发了 `MaxTrialCount` 错误。
    3.  **当前的 v1.2 架构 (批量打包)**: 将一个批次（例如50个页面）的所有创建操作，全部包裹在**一个** `invokeGroup` 中。**结果**：解决了 `MaxTrialCount` 错误，实现了稳定的批量导入。**这是目前最稳定可靠的底层导入架构。**

## 6. 如何接手并解决问题 (未来怎么办？)
当前插件效果不佳的**唯一原因**，是 `importer.ts` 中的 `parseContentToFragments` 函数实现得过于简陋。后续工作应完全集中在此。

**建议路线图：**
1.  **目标**: 精确地将 Logseq Markdown 文本，翻译成虎鲸的 `ContentFragment[]` 数组。
2.  **方法**: 放弃简单的正则表达式，在 `parseContentToFragments` 中实现一个“微型解析器”。
    *   遍历内容字符串，根据 `[[`、`((`、`![` 等特殊标记，将字符串切分成不同的“Token”（纯文本、链接、引用、附件等）。
    *   为每一种 Token 类型，创建一个对应的、符合虎鲸 `ContentFragment` 格式的对象。
    *   例如，对于 `[[页面链接]]`，应该生成 `{ t: "r", v: "页面链接" }`。
3.  **高优先级任务**:
    *   **处理块引用/嵌入**:
        *   当解析到 `((uuid))` 时，利用传入的 `graph` 对象 (`graph.blocks.get(uuid)`) 来查找被引用的块。
        *   理想情况是找到虎鲸 `ContentFragment` 中代表“引用”的类型并使用。如果找不到，退而求其次，生成一个包含被引用块内容的文本片段，如 `{ t: "t", v: `(引用) ${foundBlock.content}` }`，这也能极大提升可读性。
    *   **处理附件**:
        *   解析出附件路径后，可能需要调用 `orca.invokeBackend("upload-asset-binary", ...)` 将其上传到虎鲸笔记，然后将返回的新路径嵌入到笔记中。这需要更复杂的异步流程。

**总结：底层的批量导入架构已经稳定。现在所有的工作都集中在提升内容解析的精确度上。**

感谢您的时间和耐心，希望这份文档能为后续的开发提供清晰的指引。
